{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Assignment"
      ],
      "metadata": {
        "id": "pFOkSTNsn21Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "  -> Information Gain is a metric derived from information theory used to quantify how much ‚Äúinformation‚Äù (i.e., reduction in uncertainty) is obtained by splitting a dataset by a particular attribute (feature).\n",
        "\n",
        "  How is Information Gain used in Decision Trees?\n",
        "\n",
        "In the context of decision tree learning (e.g., algorithms like ID3, C4.5), Information Gain is used as a splitting criterion to choose which attribute to split on at each node. Here is how:\n",
        "\n",
        "At a given node in the tree, you have a subset of training examples.\n",
        "\n",
        "For each candidate attribute (that hasn‚Äôt been used yet in the path), compute the IG of splitting on that attribute.\n",
        "\n",
        "Choose the attribute with the highest Information Gain ‚Äî i.e., the one that gives the largest reduction in entropy (most ‚Äúpure‚Äù or homogeneous split) for that node.\n",
        "\n",
        "\n",
        "Create child nodes corresponding to the values of that attribute, and repeat recursively on each child subset until stopping criteria (e.g., all examples in subset belong to one class, or no attributes remain)\n",
        "\n",
        "Because each split ideally reduces uncertainty (increases class homogeneity), the tree becomes more confident in its predictions as you go down."
      ],
      "metadata": {
        "id": "ccBZzsrooB6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "  -> Gini is computationally simpler/faster (because no log calculations) so many implementations use it by default.\n",
        "\n",
        "\n",
        "Entropy is more sensitive to changes in class probabilities (especially smaller probabilities) ‚Äî in practice though the difference in tree performance is often minimal.\n",
        "\n",
        "\n",
        "Choice between them often doesn‚Äôt make a big practical difference; tuning other parameters (pruning, depth) often matters more."
      ],
      "metadata": {
        "id": "WPMMRzA1ovhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3:What is Pre-Pruning in Decision Trees?\n",
        "  -> Pre-pruning means stopping the growth of the decision tree before it becomes fully grown (i.e., before every possible split is made) by using certain stopping criteria. In other words, while building the tree you check whether further splitting is justified; if not, you stop and make the current node a leaf."
      ],
      "metadata": {
        "id": "qdLIXib1o_oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "PP6UKk2kpijb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 2. Load sample data (you can replace this with your own)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# 3. Split into train / test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Build classifier with Gini Impurity criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# 6. Print feature importances alongside feature names\n",
        "feat_imp = pd.Series(importances, index=X.columns)\n",
        "feat_imp = feat_imp.sort_values(ascending=False)\n",
        "print(\"Feature importances (descending):\")\n",
        "print(feat_imp)\n",
        "\n",
        "# 7. (Optional) Evaluate accuracy\n",
        "print(\"\\nTest set accuracy: {:.3f}\".format(clf.score(X_test, y_test)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7adNQFUoBby",
        "outputId": "a1edb963-aa95-4a1f-bb45-1a13be476e21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances (descending):\n",
            "petal length (cm)    0.893264\n",
            "petal width (cm)     0.087626\n",
            "sepal width (cm)     0.019110\n",
            "sepal length (cm)    0.000000\n",
            "dtype: float64\n",
            "\n",
            "Test set accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 5: What is a Support Vector Machine (SVM)?\n",
        "->A Support Vector Machine (SVM) is a supervised machine-learning algorithm that finds the best boundary (hyperplane) between classes by maximising the margin between the closest data points of different classes.\n",
        "\n",
        "\n",
        "Key points:\n",
        "\n",
        "It works for classification (and can be extended to regression).\n",
        "\n",
        "\n",
        "If the data isn‚Äôt linearly separable, it uses a kernel function to map data into a higher-dimensional space where separation is possible.\n",
        "\n",
        "\n",
        "The ‚Äúsupport vectors‚Äù are the training points closest to the decision boundary; they determine the position of the boundary.\n"
      ],
      "metadata": {
        "id": "U25xV7uWpxkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: What is the Kernel Trick in SVM?\n",
        " -> The kernel trick is a technique used in SVMs to enable them to handle non-linearly separable data by mapping it into a higher-dimensional feature space ‚Äî but without ever explicitly computing that mapping.\n",
        "\n",
        "\n",
        "In simpler terms: you replace the usual dot-product operations in the SVM algorithm with a kernel function\n",
        "ùêæ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë•\n",
        "ùëó\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ",x\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        ") that computes the dot product of the features in the transformed (higher) space:\n",
        "\n",
        "ùêæ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë•\n",
        "ùëó\n",
        ")\n",
        "=\n",
        "ùúô\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ")\n",
        "‚ãÖ\n",
        "ùúô\n",
        "(\n",
        "ùë•\n",
        "ùëó\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ",x\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        ")=œï(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")‚ãÖœï(x\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "where\n",
        "ùúô\n",
        "(\n",
        "‚ãÖ\n",
        ")\n",
        "œï(‚ãÖ) is the (possibly very high dimensional) mapping function."
      ],
      "metadata": {
        "id": "FFZ5KjxFqNPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q33e2su6qeei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load data\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Feature scale (important for SVMs)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train SVM with linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_lin = svm_linear.predict(X_test_scaled)\n",
        "acc_lin = accuracy_score(y_test, y_pred_lin)\n",
        "\n",
        "# 5. Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 6. Print results\n",
        "print(f\"Accuracy with Linear kernel   : {acc_lin:.4f}\")\n",
        "print(f\"Accuracy with RBF kernel      : {acc_rbf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ3DsoEppwtl",
        "outputId": "f1af9cee-a75c-4dad-ea54-0592d45a2cc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear kernel   : 0.9630\n",
            "Accuracy with RBF kernel      : 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        " -> The Na√Øve Bayes classifier is a probabilistic supervised learning algorithm used for classification. It is based on Bayes‚Äô theorem, which relates the probability of a class given the features to the prior probability of the class and the likelihood of the features given the class. It computes for each class\n",
        "ùê∂\n",
        "ùëò\n",
        "C\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        " and a feature vector\n",
        "ùë•\n",
        "=\n",
        "(\n",
        "ùë•\n",
        "1\n",
        ",\n",
        "ùë•\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùë•\n",
        "ùëõ\n",
        ")\n",
        "x=(x\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        ",x\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        ",‚Ä¶,x\n",
        "n\n",
        "\t‚Äã\n",
        "\n",
        "):\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "ùëò\n",
        "‚à£\n",
        "ùë•\n",
        ")\n",
        "‚àù\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "ùëò\n",
        ")\n",
        "\n",
        "‚àè\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùê∂\n",
        "ùëò\n",
        ")\n",
        "P(C\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        "‚à£x)‚àùP(C\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "i=1\n",
        "‚àè\n",
        "n\n",
        "\t‚Äã\n",
        "\n",
        "P(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚à£C\n",
        "k\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "and assigns the class with the highest posterior probability.\n",
        "Wikipedia\n",
        "+2\n",
        "GeeksforGeeks\n",
        "+2\n",
        "\n",
        "Why is it called ‚ÄúNa√Øve‚Äù?\n",
        "It is called ‚Äúna√Øve‚Äù because of a strong simplifying assumption: it assumes that all features (the\n",
        "ùë•\n",
        "ùëñ\n",
        "x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ") are conditionally independent of each other given the class label. In reality, features often are correlated‚Äîbut the classifier ‚Äúna√Øvely‚Äù ignores those inter-dependencies.\n",
        "\n",
        "If you like, I can also provide a short Python example of how to implement Na√Øve Bayes (e.g., with text data) and show when its ‚Äúna√Øve‚Äù assumption may matter."
      ],
      "metadata": {
        "id": "1GeO5Lxqqy1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        " -> Use Gaussian NB when your features are real-valued and roughly normally distributed.\n",
        "\n",
        "Use Multinomial NB when your features count things (how many times something occurred).\n",
        "\n",
        "Use Bernoulli NB when your features are binary indicators (whether something happened or not)."
      ],
      "metadata": {
        "id": "nnhpJgoSrIWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n"
      ],
      "metadata": {
        "id": "Pwf5Nzmsr901"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Load the data\n",
        "data = load_breast_cancer()\n",
        "X   = data.data\n",
        "y   = data.target\n",
        "\n",
        "# 3. Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,     # 30% test size\n",
        "    random_state=42,   # for reproducibility\n",
        "    stratify=y         # maintain class balance in split\n",
        ")\n",
        "\n",
        "# 4. Initialize the Gaussian Naive Bayes classifier\n",
        "clf = GaussianNB()\n",
        "\n",
        "# 5. Train (fit) the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 7. Compute accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of GaussianNB on Breast Cancer dataset: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "3hnYPb1SqvCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbz8fT-osJ2P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}